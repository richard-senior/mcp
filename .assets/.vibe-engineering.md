# AI

## Introduction

### 101: What exactly is an LLM (Large Language Model)?

Let me ask you a question, what comes next? :
```text
2, 4, 6, 8 ... ?
```

Your answer is '8' right?

```text
WRONG. Try again
```

Ok this time your answer is 'who do we appreciate' right?

```text
WRONG!
```

Ok, the answer was: 'little too late', it's a line from the 80's song "2-4-6-8 Motorway" by "Tom Robinson Band"

Now if I had have given you a bit of context like:
```text
In terms of song lyrics, what comes after "2, 4, 6, 8" ?
```
You may (if you knew the song) have been able to answer correctly, but how?

As I'm speaking to you and I say 'In terms of song lyrics'.. your brain is reconfiguring itself, it knows that what is about to come is some question or statement relating to song lyrics. When the question eventually comes your brain is ready to answer. It's ready to guess
the what comes next.

And this is how LLM's work.
Somewhere in the LLM is a 'node' representing word (parameter) 'Two'... And the 'two' node has links to everything that might possibly follow the word 'Two'.


This it has learned by reading exobytes of data, and configuring nodes that link together, such that it knows what may or may not follow every 'parameter'.


Some of these links to what comes next get more or less likely based on what has come before, such that when the LLM is responding it is essentially choosing (based on what has already been said) the most likely next word.

There's a lot more to it than that.. but that, is essentially how an LLM works.
And this constant attempt to grasp the context of the conversation is important.

### What is an 'Agentic' LLM?
What gives a human agency?
Well humans have memory and sensory inputs that constantly provide new context:
* I can see and hear that a car is coming, so I probably won't just walk out into the road.
* I remember that I did this last time and it didn't work so I'll try something different
And so it is with Agentic LLM's.


Practically all LLM's are 'generative'. That is they 'generate' some output based upon some input.
Agentic (meaning something that has 'agency') LLM's are essentially just AI's that can 'generate' output
based not only on what they currently know, but on new information (context) and feedback from its own output (memory).
This allows them to operate more autonomously (with agency).

Agentic LLM's are essentially generative LLM's that have some extra complexity allowing them to divide
up large tasks, remember recent events, use tools (like our eyes) to generate fresh context etc.

The current state of the art (that the general public are aware of) is 'Reasoning Models', which are
Agentic LLM's that have hueristics allowing them to 'choose' between several possible responses.
One such is DeepSeek R1.

The pace of development in AI is now so fast that nobody really knows what will be state of the art in 12 months.
Perhaps the greatest living British scientist Roger Penrose doesn't believe that Artificial General Intelligence is
possible, but apparently Jimi Hendrix wasn't that good on the trumpet.




